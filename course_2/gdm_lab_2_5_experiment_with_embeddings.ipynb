{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLS7I5841kr5"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COuewjQm1mWI"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C2-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQLNwxCE1r53"
      },
      "source": [
        "# Lab: Experiment with Embeddings\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_5_experiment_with_embeddings.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Explore how meaning is encoded in the high-dimensional token embeddings of the Gemma language model.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eqlhpTJddqq"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In the previous activities, you have recognized that **token embeddings** are high-dimensional vectors. You have also investigated how good embeddings map tokens into this high-dimensional space in such a way that tokens with similar meaning are close together.\n",
        "\n",
        "In this lab, you will explore these concepts in practice. You will work with embeddings for some of the tokens in Gemma and assess their similarity by computing the **cosine similarity** between pairs of token embeddings. You will also experiment with different methods for visualizing 1,152-dimensional embeddings from the Gemma model in two dimensions.\n",
        "\n",
        "Working with embeddings (and neural networks, more generally) often involves dealing with vectors and matrices. This lab includes a short introduction to the [`numpy`](https://numpy.org/) package which implements data types and common functions for vectors and matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSWk40EA-k35"
      },
      "source": [
        "\n",
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "\n",
        "* How the embedding layer of an LLM (e.g., Gemma) maps tokens to high-dimensional vectors.\n",
        "* How to work with vectors and matrices in Python using the `numpy` package.\n",
        "* The geometry of dot products and why cosine similarity captures angle-based semantic closeness.\n",
        "* The limits of inspecting single embedding dimensions and how dimensionality reduction helps.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Load a part of the Gemma embedding matrix.\n",
        "* Implement functions to extract embeddings from the embedding matrix and compute dot products.\n",
        "* Implement a function that prints similarities for pairs of tokens.\n",
        "* Visualize individual embedding dimensions.\n",
        "* Experiment with dimensionality-reduction techniques such as t-SNE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Mc6UzdSbSJ"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJikYa4iScuY"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQunPUsRShVa"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKpR1cc0Se9B"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6cTA8GKUba2"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily work with the `numpy` package (imported as `np`) for working with vectors and matrices. You will also use functions from the custom `ai_foundations` package for visualizing embeddings and performing dimensionality reduction.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYksWTRhUjVm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import numpy as np # For working with vectors and matrices.\n",
        "# For loading and projecting embeddings.\n",
        "from ai_foundations import embeddings as emb\n",
        "# For providing feedback.\n",
        "from ai_foundations.feedback.course_2 import embeddings as emb_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S7BGEPNbn2i"
      },
      "source": [
        "## Load Gemma embeddings\n",
        "\n",
        "As a first step, load the Gemma embeddings for 24 tokens.\n",
        "\n",
        "You have encountered in one of the previous labs that Gemma's tokenizer uses a vocabulary of more than 260,000 tokens. Its embedding table therefore also has entries for more than 260,000 tokens. To reduce the memory requirements and speed up computations, you will work only with 24 token embeddings in this lab.\n",
        "\n",
        "Run the following cell to load the embeddings and print the associated list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zdv4lprFPK8"
      },
      "outputs": [],
      "source": [
        "# Load Gemma embeddings.\n",
        "embeddings, labels = emb.load_gemma_embeddings(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/gemma_embeddings.npz\")\n",
        "\n",
        "print(f\"The number of tokens are {len(labels)}.\")\n",
        "print(f\"The token labels are:\\n  {'\\n  '.join(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUljfKVqihl"
      },
      "source": [
        "## Working with matrices and vectors using `numpy`\n",
        "\n",
        "------\n",
        ">\n",
        "> **â„¹ï¸ Info: Vectors and matrices in Python**\n",
        ">\n",
        ">The table that stores the embeddings is usually a matrix where each row is a vector that contains the embedding for one token. Before you explore the properties of the Gemma embeddings, explore how you can work with vectors and matrices in Python. This is important for not only working with embeddings but also many other aspects of machine learning, as almost every machine learning model involves a lot of vector and matrix operations.\n",
        ">\n",
        ">A vector is in theory just an ordered list of numbers. In theory, you could therefore define vectors as Python lists. Analogously, you could define a matrix as a list of vectors, so a list of lists. However, this would require a lot of steps to perform common vector and matrix operations, such as the dot product or matrix multiplications. In practice, it is therefore a lot more convenient to use the `numpy` package for working with vectors and matrices.\n",
        ">\n",
        ">\n",
        ">By convention, `numpy` is usually imported as `np` as done in the imports above.\n",
        ">\n",
        ">```python\n",
        ">import numpy as np\n",
        ">```\n",
        ">\n",
        ">You can then define vectors and matrices using the `np.array` function:\n",
        ">\n",
        ">```python\n",
        "># Define a 3-dimensional vector `v` with the elements 1 2 3.\n",
        ">v = np.array([1, 2, 3])\n",
        ">\n",
        "># Define a 2x3 dimensional matrix `M` (2 rows, 3 columns) with the following elements:\n",
        ">#\n",
        ">#   6 1 4\n",
        ">#   9 0 2\n",
        ">#\n",
        ">M = np.array([[6, 1, 4], [9, 0, 2]])\n",
        ">```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmY5xgW2m_dw"
      },
      "source": [
        "### Coding Activity 1: Define vectors and matrices\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Define the following vectors and matrices using `np.array`.\n",
        ">\n",
        "> $$\\mathbf{a} = \\begin{pmatrix} 7 \\\\ 3 \\\\ 1 \\\\ 4  \\end{pmatrix} \\ \\ \\  \\ \\ \\ \\ \\mathbf{b} = \\begin{pmatrix} 1.5 \\\\ -2.5 \\end{pmatrix} \\ \\ \\ \\ \\ \\ \\mathbf{c} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix}$$\n",
        ">\n",
        "> <br>\n",
        "> $$P = \\begin{pmatrix} 7 & 4\\\\ 3 & 5 \\\\ 1 & 6 \\\\ 4 & 7  \\end{pmatrix} \\ \\ \\  \\ \\ \\ \\ Q = \\begin{pmatrix} 7 & 3 & 1 & 4 \\\\ 4 & 5 & 6 & 7 \\end{pmatrix} \\ \\ \\ \\ \\ \\ R = \\begin{pmatrix} 4 & 4 & 4 \\end{pmatrix}$$\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo2ZAK_am-id"
      },
      "outputs": [],
      "source": [
        "a = # Add your code here.\n",
        "b = # Add your code here.\n",
        "c = # Add your code here.\n",
        "\n",
        "P = # Add your code here.\n",
        "Q = # Add your code here.\n",
        "R = # Add your code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A9qi7Eoyj-Q5"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_numpy_arrays(a, b, c, P, Q, R)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYRhy8RKruko"
      },
      "source": [
        "### Coding Activity 2: The shape of vectors and matrices\n",
        "\n",
        "When working with vectors and matrices, you will often need to determine their **shape**. The shape is the dimension of a vector or a matrix. For vectors, this is the number of elements in the vector. For matrices this is the number of rows and columns in the matrix.\n",
        "\n",
        "You can get the shape of a numpy array using the `.shape` property. This returns a tuple with the number of rows and the number of columns (if it is a matrix).\n",
        "\n",
        "Run the following cell to print the vectors and matrices that you defined above along with their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJvpOd1dpPuA"
      },
      "outputs": [],
      "source": [
        "print(f\"a = {a}\")\n",
        "print(f\"Shape of a: {a.shape}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"b = {b}\")\n",
        "print(f\"Shape of b: {b.shape}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"c = {c}\")\n",
        "print(f\"Shape of c: {c.shape}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"P =\\n{P}\")\n",
        "print(f\"Shape of P: {P.shape}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"Q =\\n{Q}\")\n",
        "print(f\"Shape of Q: {Q.shape}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "print(f\"R =\\n{R}\")\n",
        "print(f\"Shape of R: {R.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFLFk7MYuMjo"
      },
      "source": [
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the following cell to compute the dimension of each token embedding in `embeddings`.\n",
        ">\n",
        "> Hint: The dimension of each vector is the number of columns in the `embeddings` matrix.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hF4Kegruxve"
      },
      "outputs": [],
      "source": [
        "embedding_dim = # Your code here.\n",
        "\n",
        "print(f\"Dimension of Gemma embeddings: {embedding_dim:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "th1FTRe6kSL6"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_embedding_dimension(embedding_dim, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlbxt-6vwgSM"
      },
      "source": [
        "### Coding Activity 3: Access rows and columns\n",
        "\n",
        "One advantage of working with `numpy` arrays is that you can easily access individual rows or columns of the matrix.\n",
        "\n",
        "To access parts of a matrix `M`, you can use angled brackets, like you would with lists:\n",
        "\n",
        "```python\n",
        "M[row_or_rows, column_or_columns]\n",
        "```\n",
        "\n",
        "If you want all rows or all columns, you can use the placeholder `:`.\n",
        "\n",
        "For example, the following code accesses all columns of the 3rd row  of the matrix `M`:\n",
        "\n",
        "```python\n",
        "M[2, :]\n",
        "```\n",
        "\n",
        "The following code accesses all rows of the 4th column of `M`:\n",
        "\n",
        "```python\n",
        "M[:, 3]\n",
        "```\n",
        "\n",
        "Note, that as usual in Python, the first row and column has the index 0.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Access the third row (index 2) and the seventh column (index 6) from `embeddings`.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fvj28Cp2TiN"
      },
      "outputs": [],
      "source": [
        "third_row = # Add your code here.\n",
        "seventh_column = # Add your code here.\n",
        "\n",
        "print(f\"Shape of third_row: {third_row.shape}\")\n",
        "print(f\"Shape of seventh_column: {seventh_column.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CQupiOv-kd4R"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_numpy_slicing(third_row, seventh_column, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65zr8RPe2hb6"
      },
      "source": [
        "### Coding Activity 4: Dot product\n",
        "\n",
        "The dot product is one of the most common operations you will use in machine learning. This product (also called the inner product in this context) between $K$-dimensional vectors $\\mathbf{u} \\in \\mathbb{R}^K$ and $\\mathbf{v} \\in \\mathbb{R}^K$ is defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{k=1}^K u_k v_k\n",
        "\\;=\\;\n",
        "u_1 v_1 + u_2 v_2 + \\cdots + u_K v_K $$\n",
        "\n",
        "\n",
        "It is also sometimes written as $\\mathbf{u}^T \\mathbf{v}$. The superscript ${}^T$ indicates that the vector should be transposed, that is, a column vector should be transformed into a row vector.\n",
        "\n",
        "In Python, you can compute the dot product between the vectors `u` and `v` using either the `np.dot` function, or the more general  `np.matmul` function that is used to multiply matrices:\n",
        "\n",
        "```python\n",
        "dot_product = np.dot(u, v)\n",
        "\n",
        "dot_product = np.matmul(u.T, v)\n",
        "```\n",
        "\n",
        "Note that if you use matmul, you have to make sure that the second dimension of the first argument and the first dimension of the second argument agree. This may involve computing the transpose, which is done here using `u.T`.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the following cell to compute the dot product between the third row (index 2) and the fourth row (index 3) of `embeddings`.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcxBQ81i8uX-"
      },
      "outputs": [],
      "source": [
        "third_row = # Add your code here.\n",
        "fourth_row = # Add your code here.\n",
        "\n",
        "dot_product = # Add your code here.\n",
        "\n",
        "print(f\"Dot product: {dot_product:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O5NrBh9Lkp2_"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_dot_product(dot_product, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3ZSkTKtdEQS"
      },
      "source": [
        "\n",
        "\n",
        "In the context of embeddings, the dot product is an important measure of similarity:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/inner-products.png)"
      ],
      "metadata": {
        "id": "twCfCiTh_LX7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KE9aB1YNgad"
      },
      "source": [
        "* If the dot product is negative, that is, when $\\mathbf{u}^T \\mathbf{v} < 0$, the angle between them is bigger than 90 degrees. This means that the two vectors are pointing in opposite directions and this indicates a high level of dissimilarity.\n",
        "\n",
        "* If the dot product between two embeddings is zero, that is, when $\\mathbf{u}^T \\mathbf{v} = 0$, they are orthogonal and the angle between them is 90 degrees. This means that, loosely speaking, the embeddings are unrelated.\n",
        "\n",
        "* If the dot product is positive, that is, when $\\mathbf{u}^T \\mathbf{v} > 0$, the angle between them is less than 90 degrees. This means that the vectors are pointing in a similar direction and this indicates that the embeddings are similar.\n",
        "\n",
        "When you placed the embeddings for \"apple\" and \"banana\" on a 2D plane, you most likely placed them so that the angle between the two embeddings is small, and intuitively placed them so that $\\mathbf{u}^T \\mathbf{v} > 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNToDjol1Fjm"
      },
      "source": [
        "## Cosine similarity\n",
        "\n",
        "While the dot product by itself already indicates the similarity of two embeddings, it can become very big or very small when a vector has many dimensions. This is because you are summing over a lot of values. To make the similarities less dependent on the specific values and the number of dimensions, it can be useful to **normalize** the similarity such that it always returns a value between -1 and +1.\n",
        "\n",
        "The **cosine similarity** does exactly that:\n",
        "\n",
        "$$\n",
        "\\text{cosine}\\ \\bigl(\\mathbf{u},\\mathbf{v}\\bigr)\n",
        "\\;=\\;\n",
        "\\frac{\\mathbf{u}\\,\\cdot\\,\\mathbf{v}}\n",
        "     {\\lVert \\mathbf{u} \\rVert \\,\\lVert \\mathbf{v} \\rVert}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{u}\\cdot\\mathbf{v}$ is the dot product of the two vectors, and ${\\lVert \\mathbf{u} \\rVert \\,\\lVert \\mathbf{v} \\rVert}$\n",
        "are the magnitudes (lengths) of the vectors $\\mathbf{u}$ and $\\mathbf{v}$, respectively.\n",
        "\n",
        "As you can observe in the formula, the cosine similarity measures how similar two vectors are by computing the dot product between them, scaled by their lengths. This captures the cosine of the angle between them rather than their magnitude.\n",
        "\n",
        "The cosine similarity is +1 for identical directions, 0 for orthogonal vectors (e.g., embeddings of unrelated tokens), and -1 for opposite vectors (e.g., embeddings of strong antonyms).\n",
        "\n",
        "Run the following cell to define a function that computes the cosine similarity between two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJHsuVrdTE-A"
      },
      "outputs": [],
      "source": [
        "def cos_sim(u: np.ndarray, v: np.ndarray) -> float:\n",
        "    \"\"\"Computes the cosine similarity between two 1-D numpy arrays u and v.\n",
        "\n",
        "    Args:\n",
        "      u: A vector of dimension (k,).\n",
        "      v: A vector of dimension (k,).\n",
        "\n",
        "    Returns:\n",
        "      The dot product between u and v.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_uv = np.matmul(u.T,v)\n",
        "\n",
        "    # np.linalg.norm(u, 2) computes the length of the vector u (its L2-norm).\n",
        "    len_u = np.linalg.norm(u, 2)\n",
        "    len_v = np.linalg.norm(v, 2)\n",
        "\n",
        "    # u . v / (||u|| * ||v||).\n",
        "    cosine_sim = dot_uv / (len_u * len_v)\n",
        "\n",
        "    # Turn 1x1 numpy array into a float.\n",
        "    cosine_sim = cosine_sim.item()\n",
        "\n",
        "    return cosine_sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu2lF2y4A_HQ"
      },
      "source": [
        "### Coding Activity 5: Access the embedding for a token\n",
        "\n",
        "Before you can compute the cosine similarity between two token embeddings, you need to write a function that returns the embedding for a specific token, e.g., \"apple\".\n",
        "\n",
        "For this you need to determine the index of the token in the embedding matrix. The list `labels` that was loaded at the top of this lab contains all tokens with corresponding embeddings in `embeddings`. The embedding of the first element in the list is the first row of `embeddings`, the embedding of the second element in the list is the second `embedding`, etc.\n",
        "\n",
        "To determine the index of the embedding, you can use the `.index` method of the list. For example, `labels.index(\"apple\")` returns the index of the row of the embedding for \"apple\".\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `get_embedding(token)` in the cell below.\n",
        ">\n",
        "> It should look up the index of the token in `labels` and then return the token embedding for `token`.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNA-caJfL9iR"
      },
      "outputs": [],
      "source": [
        "def get_embedding(\n",
        "    token: str, embeddings: np.ndarray = embeddings, labels: list[str] = labels\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Returns the embedding for `token` from `embeddings`.\n",
        "\n",
        "    Args:\n",
        "      token: The token for which the embedding should be retrieved.\n",
        "      embeddings: The embedding matrix with embeddings for all tokens in\n",
        "        `labels`.\n",
        "      labels: The list of tokens indicating the order of embeddings in\n",
        "        `embeddings`.\n",
        "\n",
        "    Returns:\n",
        "      The token embedding (a vector) for `token`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError if no embeddings for `token` exists.\n",
        "    \"\"\"\n",
        "\n",
        "    if token not in labels:\n",
        "        raise ValueError(f\"No embeddings for {token} exist.\")\n",
        "\n",
        "    token_idx =  # Add your code here.\n",
        "    embedding =  # Add your code here.\n",
        "\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rBZzD0T5kwUX"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_get_embedding(get_embedding, embeddings, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1f0R6-LDsG6"
      },
      "source": [
        "### Coding Activity 6: Compute the cosine similarity\n",
        "\n",
        "You will now use the implementation of `cos_sim` to define a function that prints the similarity between the embeddings of two tokens.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `print_similarity` below.\n",
        ">\n",
        "> It should use your implementation of `get_embedding` to retrieve the embeddings for `token1` and `token2`, and use the implementation of `cos_sim` above to compute the cosine similarity.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NCLTynBVlDA"
      },
      "outputs": [],
      "source": [
        "def print_similarity(\n",
        "    token1: str,\n",
        "    token2: str,\n",
        "    embeddings: np.ndarray = embeddings,\n",
        "    labels: list[str] = labels,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes and prints the cosine similarity between the embeddings of `token1`\n",
        "      and `token2`.\n",
        "\n",
        "    Args:\n",
        "      token1: The first token for the similarity computation.\n",
        "      token2: The second token for the similarity computation.\n",
        "      embeddings: The embedding matrix with embeddings for `token1` and\n",
        "        `token2`.\n",
        "      labels: The list of tokens indicating the order of embeddings in\n",
        "        `embeddings`.\n",
        "\n",
        "    Returns:\n",
        "      The cosine similarity between `token1` and `token2`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError if no embedding for `token1` or `token2` exists.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    embedding1 = # Add your code here.\n",
        "    embedding2 = # Add your code here.\n",
        "\n",
        "    similarity = # Add your code here.\n",
        "    print(\n",
        "        f'Cosine similarity between \"{token1}\" and \"{token2}\" '\n",
        "        f'\\t= {similarity:.2f}'\n",
        "    )\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GXHLu4aVk7eZ"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your code\n",
        "emb_feedback.test_print_similarity(print_similarity, get_embedding, cos_sim, embeddings, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MxwWntRaes"
      },
      "source": [
        "You can now use this function to compute the similarity between pairs of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WalfMXN-QbIq"
      },
      "outputs": [],
      "source": [
        "print_similarity(\"king\", \"king\")\n",
        "print_similarity(\"king\", \"queen\")\n",
        "print_similarity(\"queen\", \"king\")\n",
        "print_similarity(\"joy\", \"happy\")\n",
        "print_similarity(\"good\", \"bad\")\n",
        "print_similarity(\"sad\", \"happy\")\n",
        "print_similarity(\"king\", \"bus\")\n",
        "print_similarity(\"car\", \"banana\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7dn8-0dSuMa"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "In the embedding space of a well-trained model such as Gemma, you expect to see similarities between closely related tokens to be high. This is the case here, for example the similarity between \"king\" and \"queen\" is 0.42. This indicates that the model has learned that these two tokens should be embedded close together and should have a high semantic similarity. Also note how the cosine similarity is symmetric. The similarity between \"king\" and \"queen\" and the similarity between \"queen\" and \"king\" is exactly the same.\n",
        "\n",
        "The cosine similarity between entirely unrelated tokens tends to be low. For example, the similarity between \"king\" and \"bus\" is 0.04, indicating that according to the model's embedding space, these tokens are unrelated.\n",
        "\n",
        "The comparably high similarity of some of the antonyms (e.g., `cosine(sad, happy) = 0.22` or `cosine(good, bad) = 0.42`) is potentially a bit more surprising. However, even though the token pairs have opposite meanings, they are still pairs of adjectives that describe similar concepts (i.e., feelings or quality)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV8cnhy5Ku1E"
      },
      "source": [
        "## Visualizing embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WeaF6EtOubv"
      },
      "source": [
        "You will now turn to visualizing embeddings. As mentioned, the Gemma 1B model uses 1,152-dimensional embeddings. While it is straightforward to visualize two or maybe even three dimensions, visualizing a 1,152-dimensional space presents a challenge.\n",
        "\n",
        "In this part of the lab, you will explore different methods for visualizing these high-dimensional embeddings.\n",
        "\n",
        "### Visualizing individual dimensions\n",
        "\n",
        "The cell below visualizes individual dimensions of token embeddings by choosing two dimensions. For each token, this method extracts only two elements of its vector (the dimensions specified by you) and then plots the tokens in this two-dimensional space.\n",
        "\n",
        "Use the sliders below to choose different combinations of dimensions and run the cell each time to plot these two dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qxRc9ciKe_mL"
      },
      "outputs": [],
      "source": [
        "# @title Plot individual dimensions\n",
        "dimension_1 = 432  #@param {type: 'slider', min:0, max:1151}\n",
        "dimension_2 = 432  #@param {type: 'slider', min:0, max:1151}\n",
        "emb.plot_embeddings_dimensions(embeddings,\n",
        "                           labels,\n",
        "                           dim_x=dimension_1,\n",
        "                           dim_y=dimension_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juATWx6wYQdk"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "You likely found that it was difficult to find any pairs of dimensions in which clear clusters emerged. It was likely also difficult to find interpretations for the dimensions. Sometimes, one dimension may have put tokens with similar functions (e.g., nouns) closer together whereas other times, it may have put tokens with related meanings (e.g., \"banana\" and \"yellow\") closer together.\n",
        "\n",
        "In the next activity, you will use a more sophisticated dimensionality reduction technique that considers all the information in the high-dimensional embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZJQZOWTFtT1"
      },
      "source": [
        "### Visualizing  embeddings with t-SNE\n",
        "\n",
        "As you likely observed in the previous activity, it is not easy to see patterns by visualizing any two Gemma embeddings against each other. This is because meaningful word relationships typically arise from combinations of many embedding dimensions. As you are only able to visualize two or three dimensions at a time, this method did not allow you to capture the full complexity of the embedding space.\n",
        "\n",
        "To make the high-dimensional embedding space more interpretable, dimensionality reduction techniques can be of great help. Rather than naively picking two or three dimensions, these techniques try to compress the embedding space into fewer dimensions with the goal of capturing as much information as possible from the original embedding space.\n",
        "\n",
        "One such technique is [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). This technique has been particularly well-suited for projecting embeddings for visualizations. t-SNE is a visualization method that **preserves the pairwise similarities** between data points in a lower-dimensional space. This gives you an \"at a glance\" map of the neighbourhoods hidden in the high dimensional space: the algorithm keeps local distances faithful, so clusters that are closely related pop out as tight clouds that are easy to label and debug.\n",
        "\n",
        "You are then able to use these 2D points to generate a visualization that gives you an intuitive sense of how the words are distributed in the high-dimensional space. Data points close together in the high-dimensional space will appear closer together in 2D using t-SNE.\n",
        "\n",
        "Run the following cell to generate an t-SNE plot of the 22 tokens that you have been considering in this activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvJW5j9sBlfc"
      },
      "outputs": [],
      "source": [
        "emb.plot_embeddings_tsne(embeddings, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z21-ec4hKF8r"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "The plot above shows that many tokens with similar meanings are close together in the vector space. For example, there is a cluster of the tokens \"mobile\", \"phone\", \"computer\", and \"internet\". Similarly, \"man\" and \"woman\" as well as \"queen\" and \"king\" are close together.\n",
        "\n",
        "At the same time, some tokens, such as \"cat\" and \"car\" that do not have much in common are also close together. This is likely largely a result of the dimensionality reduction. By projecting the embeddings to a 2-dimensional space, you lose some fine-grained information and while it correctly projects the embeddings \"cat\" and \"dog\" close together, there are also artifacts such as the proximity between the embeddings of \"cat\" and \"car\".\n",
        "\n",
        "Recall when you manually put some of these tokens in a 2D grid. Did you end up with a similar arrangement to this? Which dimensions of meaning do you think are encoded in this plot?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqrROCzCJslm"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you explored the **high-dimensional token embeddings** of the Gemma model using the `numpy` library to perform vector and matrix operations. You implemented **cosine similarity** to mathematically measure the \"closeness\" of tokens, confirming that words with similar meanings (like \"king\" and \"queen\") are located near each other in the embedding space. You then discovered the challenge of **visualizing** these vectors, finding that plotting individual dimensions directly revealed little about the overall structure. Finally, by using a **dimensionality reduction technique** called t-SNE, you successfully projected the embeddings into 2D and observed how the model organized words into meaningful **semantic clusters**, grouping related concepts together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmOY_IP8f8l9"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bZ-LVtuf-mQ"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kam7gGxgCNn"
      },
      "outputs": [],
      "source": [
        "# Copy these solutions into the cell above.\n",
        "a = np.array([7, 3, 1, 4])\n",
        "b = np.array([1.5, -2.5])\n",
        "c = np.array([4, 4, 4])\n",
        "\n",
        "P = np.array([[7, 4], [3, 5], [1, 6], [4, 7]])\n",
        "Q = np.array([[7, 3, 1, 4], [4, 5, 6, 7]])\n",
        "R = np.array([[4, 4, 4]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z3aHz8tgCiZ"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ti42tiJgHL0"
      },
      "outputs": [],
      "source": [
        "embedding_dim = embeddings.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2xg4bT_lhL0"
      },
      "source": [
        "### Coding Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxv9YdkzljK3"
      },
      "outputs": [],
      "source": [
        "third_row = embeddings[2, :]\n",
        "seventh_column = embeddings[:, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUhnXySIljYb"
      },
      "source": [
        "### Coding Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMr574HslkDM"
      },
      "outputs": [],
      "source": [
        "third_row = embeddings[2,:]\n",
        "fourth_row = embeddings[3,:]\n",
        "\n",
        "dot_product = np.matmul(third_row.T, fourth_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPhHHzHAlkUl"
      },
      "source": [
        "### Coding Activity 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53leLWXSllaZ"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of get_embedding.\n",
        "def get_embedding(\n",
        "    token: str, embeddings: np.ndarray = embeddings, labels: list[str] = labels\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Returns the embedding for token from embeddings.\n",
        "\n",
        "    Args:\n",
        "      token: The token for which the embedding should be retrieved.\n",
        "      embeddings: The embedding matrix with embeddings for all tokens in `labels`.\n",
        "      labels: The list of tokens indicating the order of embeddings in\n",
        "        `embeddings`.\n",
        "\n",
        "    Returns:\n",
        "      The token embedding (a vector) for `token`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError if no embedding for `token` exists.\n",
        "    \"\"\"\n",
        "\n",
        "    if token not in labels:\n",
        "        raise ValueError(f\"No embeddings for {token} exist.\")\n",
        "\n",
        "    token_idx = labels.index(token)\n",
        "    embedding = embeddings[token_idx, :]\n",
        "\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UdxTvRll3Q"
      },
      "source": [
        "### Coding Activity 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyjbZBm1lmmJ"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of print_similarity.\n",
        "def print_similarity(\n",
        "    token1: str,\n",
        "    token2: str,\n",
        "    embeddings: np.ndarray = embeddings,\n",
        "    labels: list[str] = labels,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes and prints the cosine similarity between the embeddings of `token1`\n",
        "      and `token2`.\n",
        "\n",
        "    Args:\n",
        "      token1: The first token for the similarity computation.\n",
        "      token2: The second token for the similarity computation.\n",
        "      embeddings: The embedding matrix with embeddings for `token1` and `token2`.\n",
        "      labels: The list of tokens indicating the order of embeddings in\n",
        "        `embeddings`.\n",
        "\n",
        "    Returns:\n",
        "      The cosine similarity between `token1` and `token2`.\n",
        "\n",
        "    Raises:\n",
        "      ValueError if no embedding for `token1` or `token2` exists.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    embedding1 = get_embedding(token1, embeddings, labels)\n",
        "    embedding2 = get_embedding(token2, embeddings, labels)\n",
        "\n",
        "    similarity = cos_sim(embedding1, embedding2)\n",
        "    print(f'Cosine similarity between \"{token1}\" and \"{token2}\"  \\t= {similarity:.2f}')\n",
        "    return similarity\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "C8Mc6UzdSbSJ",
        "RmOY_IP8f8l9",
        "8bZ-LVtuf-mQ",
        "2Z3aHz8tgCiZ",
        "r2xg4bT_lhL0",
        "HUhnXySIljYb",
        "RPhHHzHAlkUl",
        "B8UdxTvRll3Q"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}