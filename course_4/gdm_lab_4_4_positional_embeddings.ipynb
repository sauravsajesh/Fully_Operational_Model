{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1fHZsqkyF0"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evJvwgbJk0OT"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C4-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3M5Qhjk47o"
      },
      "source": [
        "# Lab: Positional Embeddings\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_4/gdm_lab_4_4_positional_embeddings.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Explore how self-attention without positional embeddings is order-invariant.\n",
        "\n",
        "15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuOFgiPG85B6"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will explore how the attention mechanism is **position invariant**. This means the output of the attention mechanism is the same independent of the word order of the previous tokens.\n",
        "\n",
        "This notebook defines a toy attention computation. It defines random token embeddings, random query, key, and value projection matrices, and it defines the computations of a single attention head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m0KNtNmlZlA"
      },
      "source": [
        "\n",
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        "* Understand how the self-attention mechanism leads to the same output when the order of tokens in the prompt has been re-arranged.\n",
        "\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Walk through the implementation of a toy language model that implements the attention mechanism and observe which values change as you change the order of tokens in the prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvxwmPvXl82Y"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxoT_sImAQX"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gdOFghTmCzL"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMkc9qKmFXI"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaRYD3fYtnzG"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ly42FaU8tq87"
      },
      "outputs": [],
      "source": [
        "import numpy as np # For definining and working with embeddings.\n",
        "from scipy.special import softmax # For computing attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHgLAxuYuotK"
      },
      "source": [
        "## Prepare the model\n",
        "\n",
        "Run these cells one by one to compute the output of the attention head and the corresponding attention weights.\n",
        "\n",
        "The following code block first defines the sentences and the vocabulary of the model. Given that this is a simple toy model, the vocabulary is limited to the five tokens in the `vocabulary` dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KqstHo86vz57"
      },
      "outputs": [],
      "source": [
        "# Define sentences. Lower-case everything so that \"The\" and \"the\" use the same\n",
        "# embedding.\n",
        "sentence1_str = \"the zebra chased the lion .\"\n",
        "sentence2_str = \"the lion chased the zebra .\"\n",
        "\n",
        "# Define vocabulary.\n",
        "vocabulary = {\"<pad>\": 0, \"the\": 1, \"zebra\": 2, \"chased\": 3, \"lion\": 4, \".\": 5}\n",
        "inv_vocabulary = {v: k for k, v in vocabulary.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Z3xF5ou-DE"
      },
      "source": [
        "The following block constructs a random embedding matrix for the five tokens defined above. This embedding matrix will not capture any semantic similarities between words. The goal of this exercise is to demonstrate the position invariance of the attention mechanism rather than using these embeddings in a model for predicting the next token. It therefore does not matter that these embeddings are not good for making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ImNAYKEov-WL"
      },
      "outputs": [],
      "source": [
        "# Embedding dimension.\n",
        "embedding_dim = 3\n",
        "vocabulary_size = len(vocabulary)\n",
        "\n",
        "# Set a seed for reproducibility.\n",
        "np.random.seed(2311)\n",
        "\n",
        "# Embedding matrix (vocab_size x d_model).\n",
        "embedding_matrix = np.random.rand(vocabulary_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tcc33fJvm8Y"
      },
      "source": [
        "The following block similarly constructs random query, key, and value projection matrices. In a real model these matrices would be learned from data. Again, for the purpose of this exercise, it does not matter that the parameters do not represent anything useful for making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gQOjD6dywZiP"
      },
      "outputs": [],
      "source": [
        "# Dimension of key, query, value vectors (can be different from embedding_dim,\n",
        "# but here same for simplicity).\n",
        "d_k = embedding_dim\n",
        "d_q = embedding_dim\n",
        "d_v = embedding_dim\n",
        "\n",
        "# Constant for to be used in masked attention scores.\n",
        "K_MASK = -2.3819763e38  # Set to a large negative number.\n",
        "\n",
        "# Projection matrices (embedding_dim x d_k for W_q, embedding_dim x d_q for W_k,\n",
        "# embedding_dim x d_v for W_v).\n",
        "W_q = np.random.rand(embedding_dim, d_k)\n",
        "W_k = np.random.rand(embedding_dim, d_q)\n",
        "W_v = np.random.rand(embedding_dim, d_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0VQ3OWv10x"
      },
      "source": [
        "Finally, this block defines the attention computation function. It computes the queries, keys, and values for a sentence, then computes the (masked) logits, and finally the attention weights and the output of the attention head. This function also prints many intermediate computations so that you can inspect the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rEjIL7FSwfD2"
      },
      "outputs": [],
      "source": [
        "def compute_attention_output(sentence: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes the attention output and attention weights for all tokens in\n",
        "    `sentence`.\n",
        "\n",
        "    Args:\n",
        "      sentence: The sentence for which to compute the attention weights. Tokens\n",
        "        must be space-separated and from the list of tokens in vocab.\n",
        "\n",
        "    Returns:\n",
        "      attention_output: The output of the attention mechanism.\n",
        "        Shape: (num_tokens, d_v).\n",
        "      attention_weights: The attention weights for all tokens.\n",
        "        Shape: (num_tokens, num_tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = [vocabulary[word] for word in sentence.split()]\n",
        "\n",
        "    embeddings = embedding_matrix[tokens]\n",
        "\n",
        "    # Compute queries, keys, values for sentence.\n",
        "    Q = embeddings @ W_q # Shape: (num_tokens, d_q)\n",
        "    K = embeddings @ W_k # Shape: (num_tokens, d_k)\n",
        "    V = embeddings @ W_v # Shape: (num_tokens, d_v)\n",
        "\n",
        "    # Compute the attention mask.\n",
        "    l = len(tokens)\n",
        "    attention_mask = np.tri(l) # Shape: (num_tokens, num_tokens).\n",
        "    print(f\"\\n--- Sentence: \\\"{sentence}\\\" ---\")\n",
        "\n",
        "    # Compute attention logits with scaling factor.\n",
        "    scale_factor = np.sqrt(d_k)\n",
        "    logits = (Q @ K.T) / scale_factor # Shape: (num_tokens, num_tokens).\n",
        "    # Apply attention mask.\n",
        "    # Shape: (num_tokens, num_tokens).\n",
        "    logits = np.where(attention_mask, logits, K_MASK)\n",
        "\n",
        "    print(\"\\nAttention logits (Q @ K.T / sqrt(d_k)) for last token:\\n\",\n",
        "          logits[-1, :])\n",
        "\n",
        "    # Compute attention weights (SoftMax).\n",
        "    # Shape: (num_tokens, num_tokens).\n",
        "    attention_weights = softmax(logits, axis=1)\n",
        "\n",
        "    print(\"\\nAttention weights (SoftMax) for last token:\\n\",\n",
        "          attention_weights[-1, :])\n",
        "\n",
        "    print(\"\\nValue matrix:\\n\", V)\n",
        "\n",
        "    # Compute attention output.\n",
        "    attention_output = attention_weights @ V # Shape: (num_tokens, d_v)\n",
        "    print(\"\\nAttention output (weights @ V) for last token:\\n\",\n",
        "          attention_output[-1, :])\n",
        "\n",
        "    return attention_output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW9Mr0kHk98o"
      },
      "source": [
        "## Compare sentences with different word order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxpijqAHwOOP"
      },
      "source": [
        "You can now compute the attention weights and outputs for the sentences \"the zebra chased the lion .\" and \"the lion chased the zebra .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xmuYagvi3SUl",
        "outputId": "8b428c23-c22a-459b-f0e5-ff21ba036bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence: \"the zebra chased the lion .\" ---\n",
            "\n",
            "Attention logits (Q @ K.T / sqrt(d_k)) for last token:\n",
            " [0.66921786 0.68602528 0.39777211 0.66921786 1.44639615 0.89147525]\n",
            "\n",
            "Attention weights (SoftMax) for last token:\n",
            " [0.13882248 0.14117544 0.10582113 0.13882248 0.30198407 0.17337439]\n",
            "\n",
            "Value matrix:\n",
            " [[0.8021514  0.84563472 0.72789842]\n",
            " [0.76557603 0.83818417 0.76456614]\n",
            " [0.70326984 0.63636533 0.40296426]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [1.74110101 1.87565877 1.67165697]\n",
            " [1.25274724 1.23715227 0.94985545]]\n",
            "\n",
            "Attention output (weights @ V) for last token:\n",
            " [1.1481937  1.20136774 1.02217182]\n",
            "\n",
            "--- Sentence: \"the lion chased the zebra .\" ---\n",
            "\n",
            "Attention logits (Q @ K.T / sqrt(d_k)) for last token:\n",
            " [0.66921786 1.44639615 0.39777211 0.66921786 0.68602528 0.89147525]\n",
            "\n",
            "Attention weights (SoftMax) for last token:\n",
            " [0.13882248 0.30198407 0.10582113 0.13882248 0.14117544 0.17337439]\n",
            "\n",
            "Value matrix:\n",
            " [[0.8021514  0.84563472 0.72789842]\n",
            " [1.74110101 1.87565877 1.67165697]\n",
            " [0.70326984 0.63636533 0.40296426]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [0.76557603 0.83818417 0.76456614]\n",
            " [1.25274724 1.23715227 0.94985545]]\n",
            "\n",
            "Attention output (weights @ V) for last token:\n",
            " [1.1481937  1.20136774 1.02217182]\n"
          ]
        }
      ],
      "source": [
        "attention_output1, attention_weights1 = compute_attention_output(sentence1_str)\n",
        "attention_output2, attention_weights2 = compute_attention_output(sentence2_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scqUh7EbCyBc"
      },
      "source": [
        "### What did you observe?\n",
        "\n",
        "Take a close look at the attention weights and the value matrices above.\n",
        "\n",
        "As you will see, both the attention weights and the value matrices differ between the two sentences. In the attention weight vector the second and the fifth entry have been swapped (these correspond to \"zebra\" and \"lion\").\n",
        "\n",
        "The two value matrices differ in the second and the fifth row. These are the representations that correspond to \"zebra\" and \"lion\".\n",
        "\n",
        "However, when you multiply the attention vector with the matrix, then we obtain the same output in both cases. Perform this computation manually to see why.\n",
        "\n",
        "The fact that this matrix multiplication leads to the same result independent of the order of the previous tokens is the reason why the attention mechanism in its current form is order invariant.\n",
        "\n",
        "Run the following cell to see this comparison even more directly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pQX2c7rnx4S9",
        "outputId": "bcc96d6a-f7ff-4778-8952-132a0d4bed64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparison ---\n",
            "\n",
            "Attention weights for last token in S1: [0.13882248 0.14117544 0.10582113 0.13882248 0.30198407 0.17337439]\n",
            "Attention weights for last token in S2: [0.13882248 0.30198407 0.10582113 0.13882248 0.14117544 0.17337439]\n",
            "\n",
            "Attention outputs for last token in S1: [1.1481937  1.20136774 1.02217182]\n",
            "Attention outputs for last token in S2: [1.1481937  1.20136774 1.02217182]\n",
            "\n",
            "Are attention outputs for last token in S1 and S2 the same? True\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Comparison ---\")\n",
        "\n",
        "print(\"\\nAttention weights for last token in S1:\", attention_weights1[-1, :])\n",
        "print(\"Attention weights for last token in S2:\", attention_weights2[-1, :])\n",
        "\n",
        "print(\"\\nAttention outputs for last token in S1:\", attention_output1[-1, :])\n",
        "print(\"Attention outputs for last token in S2:\", attention_output2[-1, :])\n",
        "\n",
        "\n",
        "print(\"\\nAre attention outputs for last token in S1 and S2 the same?\",\n",
        "      np.allclose(attention_output1[-1, :], attention_output2[-1, :]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jll0RR0zAOLz"
      },
      "source": [
        "## Compare the attention weights for other sentences\n",
        "\n",
        "To see that this is not just an artifact of the two sentences above, perform this comparison for other pairs of sentences.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        ">**ðŸ’» Your task:**\n",
        ">\n",
        ">Compare the attention weights and outputs for other sentences.\n",
        ">\n",
        ">Note that you can only use the following tokens in your sentences (but you can use each token as often as you would like):\n",
        ">- `the`\n",
        ">- `lion`\n",
        ">- `chased`\n",
        ">- `zebra`\n",
        ">- `.`\n",
        ">\n",
        ">What happens when the set of tokens is the same across both sentences but  the last token differs? (e.g., \"lion the chased .\" and \"chased the . lion\") Does this lead to different attention outputs? If so, why?\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "7QQ6LVMc_noG",
        "outputId": "19504efc-e090-4f48-887f-ef76ed433d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sentence: \"lion the chased zebra the .\" ---\n",
            "\n",
            "Attention logits (Q @ K.T / sqrt(d_k)) for last token:\n",
            " [1.44639615 0.66921786 0.39777211 0.68602528 0.66921786 0.89147525]\n",
            "\n",
            "Attention weights (SoftMax) for last token:\n",
            " [0.30198407 0.13882248 0.10582113 0.14117544 0.13882248 0.17337439]\n",
            "\n",
            "Value matrix:\n",
            " [[1.74110101 1.87565877 1.67165697]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [0.70326984 0.63636533 0.40296426]\n",
            " [0.76557603 0.83818417 0.76456614]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [1.25274724 1.23715227 0.94985545]]\n",
            "\n",
            "Attention output (weights @ V) for last token:\n",
            " [1.1481937  1.20136774 1.02217182]\n",
            "\n",
            "--- Sentence: \"chased lion the the zebra .\" ---\n",
            "\n",
            "Attention logits (Q @ K.T / sqrt(d_k)) for last token:\n",
            " [0.39777211 1.44639615 0.66921786 0.66921786 0.68602528 0.89147525]\n",
            "\n",
            "Attention weights (SoftMax) for last token:\n",
            " [0.10582113 0.30198407 0.13882248 0.13882248 0.14117544 0.17337439]\n",
            "\n",
            "Value matrix:\n",
            " [[0.70326984 0.63636533 0.40296426]\n",
            " [1.74110101 1.87565877 1.67165697]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [0.8021514  0.84563472 0.72789842]\n",
            " [0.76557603 0.83818417 0.76456614]\n",
            " [1.25274724 1.23715227 0.94985545]]\n",
            "\n",
            "Attention output (weights @ V) for last token:\n",
            " [1.1481937  1.20136774 1.02217182]\n",
            "\n",
            "--- Comparison ---\n",
            "\n",
            "Attention weights for last token in S1: [0.30198407 0.13882248 0.10582113 0.14117544 0.13882248 0.17337439]\n",
            "Attention weights for last token in S2: [0.10582113 0.30198407 0.13882248 0.13882248 0.14117544 0.17337439]\n",
            "\n",
            "Attention outputs for last token in S1: [1.1481937  1.20136774 1.02217182]\n",
            "Attention outputs for last token in S2: [1.1481937  1.20136774 1.02217182]\n",
            "\n",
            "Are attention outputs for last token in S1 and S2 the same? True\n"
          ]
        }
      ],
      "source": [
        "# @title Compare attention weights and outputs for other sentences\n",
        "\n",
        "sentence_1 = \"lion the chased zebra the .\"  # @param {\"type\": \"string\"}\n",
        "sentence_2 = \"chased lion the the zebra .\"  # @param {\"type\": \"string\"}\n",
        "\n",
        "tokens_1 = sentence_1.split()\n",
        "tokens_2 = sentence_2.split()\n",
        "\n",
        "possible_token_list = \"', '\".join(vocabulary.keys())\n",
        "\n",
        "for t in tokens_1:\n",
        "    if t not in vocabulary:\n",
        "        raise ValueError(\n",
        "            f\"Invalid token '{t}' in sentence_1. Please only use one of the\"\n",
        "            f\" following tokens: '{possible_token_list}'.\"\n",
        "        )\n",
        "\n",
        "for t in tokens_2:\n",
        "    if t not in vocabulary:\n",
        "        raise ValueError(\n",
        "            f\"Invalid token '{t}' in sentence_2. Please only use one of the\"\n",
        "            f\" following tokens: '{possible_token_list}'.\"\n",
        "        )\n",
        "\n",
        "attention_output1, attention_weights1 = compute_attention_output(sentence_1)\n",
        "attention_output2, attention_weights2 = compute_attention_output(sentence_2)\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "\n",
        "print(\"\\nAttention weights for last token in S1:\", attention_weights1[-1, :])\n",
        "print(\"Attention weights for last token in S2:\", attention_weights2[-1, :])\n",
        "\n",
        "print(\"\\nAttention outputs for last token in S1:\", attention_output1[-1, :])\n",
        "print(\"Attention outputs for last token in S2:\", attention_output2[-1, :])\n",
        "\n",
        "print(\n",
        "    \"\\nAre attention outputs for last token in S1 and S2 the same?\",\n",
        "    np.allclose(attention_output1[-1, :], attention_output2[-1, :]),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9WuEQ9iv-ez"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This interactive activity showed you that the output of the attention mechanism for the last token (that is, the embedding from which the model predicts the next token) does not depend on the order of tokens in the prompt. The attention mechanism is therefore **order-invariant**.\n",
        "\n",
        "In the next activity, you will explore techniques for encoding positional information in transformer models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wvxwmPvXl82Y"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}