{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McySAWvt09wn"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ],
      "metadata": {
        "id": "bg_nGpxOxaPv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bW1AyB_f7eo"
      },
      "source": [
        "# Lab: Train Your Own Small Language Model\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_5_train_your_own_small_language_model.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>     \n",
        "\n",
        "40 minutes\n",
        "\n",
        "Train a transformer language model on the Africa Galore dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will make use of the data pre-processing steps that you have implememented in the previous lab and prepare the data to be used for training a transformer model. You will then train your own small language model on the Africa Galore dataset and explore its predictions. The model you will be training is referred to as a small language model because it has comparably fewer parameters (around 3.5 million instead of the approximately 1 billion of Gemma-1B) and will be trained on the small Africa Galore dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "md4FSU7h9dQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will know:\n",
        "\n",
        "* How to prepare a text dataset to be used for training a transformer model with Keras.\n",
        "* How to train and evaluate a small language model (SLM).\n"
      ],
      "metadata": {
        "id": "g8u6btAn9jc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "You will use an implementation of the transformer model written using [Keras](https://keras.io/). Keras is an open source deep learning framework that allows you to define neural network architectures and train models using these architectures. You will learn more about how to define models yourself in later courses. For now, you will use existing code to define the model and perform its training.\n",
        "\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset, tokenize it, and convert it to token IDs.\n",
        "* Pad the dataset such that all sequences have the same length.\n",
        "* Shuffle the examples in the dataset and group them into batches.\n",
        "* Transform the data into model inputs and model targets.\n",
        "* Train the transformer model.\n",
        "\n",
        "Note that this is quite a long lab since there are many steps that you have to go through for training a transformer language model and the training itself takes some time. If you are able to, we *highly recommend* running the code in this lab on a Colab instance with a GPU. See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n"
      ],
      "metadata": {
        "id": "qIhcVxx0foVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are excuted on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell, and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "A **GPU** is a special type of hardware that can significantly speed up some types of computations of machine learning models. Several of the activities in this lab will also run a lot faster if you run them on a GPU.\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "5.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ],
      "metadata": {
        "id": "qLvkcx5pUItk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will make use of the Keras package for defining and training the transformer model, the [Pandas](http://pandas.pydata.org) package for reading the dataset, and the [TensorFlow](https://tensorflow.org) package for shuffling the data and grouping it into batches. Run the following cell to import these packages."
      ],
      "metadata": {
        "id": "UPJE5CKOA2bJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sYqG7iVwXEX0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import os # Used for setting Keras configuration variables.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # Set a parameter for Keras.\n",
        "import re # Used for splitting text on whitespace.\n",
        "\n",
        "import keras # Used for defining an training the model.\n",
        "import pandas as pd # Used for loading the dataset.\n",
        "import tensorflow as tf # Used for shuffling the dataset.\n",
        "\n",
        "# Used for displaying nicer error messages.\n",
        "from IPython.display import display, HTML\n",
        "from ai_foundations import training # For training your model.\n",
        "from ai_foundations import generation # For prompting your model.\n",
        "from ai_foundations import visualizations # For visualizing probabilities.\n",
        "from ai_foundations.feedback.course_1 import slm # For providing feedback.\n",
        "\n",
        "# The following line provides configuration for Keras.\n",
        "keras.utils.set_random_seed(812)  # For Keras layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1XAXcZ9S-4w"
      },
      "source": [
        "## Loading and tokenizing the dataset\n",
        "\n",
        "Load the dataset. As in previous labs, you will use the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5mB31TuPSoWb",
        "outputId": "b8565f3e-9190-441d-919c-5c9970fb11d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 232 paragraphs.\n"
          ]
        }
      ],
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"].values\n",
        "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgXxQ8KGNfT"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "The following cell contains the  `SimpleWordTokenizer` class that you have encountered in the previous lab. You will use this class again to tokenize the dataset, prepare the vocabulary, and provide methods for translating tokens into token IDs and vice versa. Note that this version also adds special `<PAD>` and `<UNK>` tokens to the vocabulary. You will learn more about the purpose of these special tokens as part of this lab.\n",
        "\n",
        "Run the following cell to define the `SimpleWordTokenizer` and tokenize the Africa Galore dataset, and translate its tokens to IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LMK-adL0su6i"
      },
      "outputs": [],
      "source": [
        "class SimpleWordTokenizer:\n",
        "    \"\"\"A simple word tokenizer.\n",
        "\n",
        "    The tokenizer splits the text sequence based on whitespace, using the\n",
        "    `encode` method to convert the text into a sequence of indices and the\n",
        "    `decode` method to convert indices back into text.\n",
        "\n",
        "    The simple word tokenizer that can be initialized with a corpus or using a\n",
        "    provided vocabulary list\n",
        "\n",
        "    Typical usage example:\n",
        "\n",
        "        corpus = \"Hello there!\"\n",
        "        tokenizer = SimpleWordTokenizer(text)\n",
        "        print(tokenizer.encode('Hello'))\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define constants.\n",
        "    UNKNOWN_TOKEN = \"<UNK>\"\n",
        "    PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
        "\n",
        "        Args:\n",
        "          corpus: Input text dataset.\n",
        "          vocabulary: A pre-defined vocabulary. If None,\n",
        "              the vocabulary is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "\n",
        "        if vocabulary is None:\n",
        "            # Build the vocabulary from scratch.\n",
        "            if isinstance(corpus, str):\n",
        "                corpus = [corpus]\n",
        "\n",
        "            # Convert text sequence to tokens.\n",
        "            tokens = []\n",
        "            for text in corpus:\n",
        "                for token in self.space_tokenize(text):\n",
        "                    tokens.append(token)\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            vocabulary = self.build_vocabulary(tokens)\n",
        "\n",
        "            # Add special unknown and pad tokens to the vocabulary list.\n",
        "            self.vocabulary = (\n",
        "                [self.PAD_TOKEN] + vocabulary + [self.UNKNOWN_TOKEN]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
        "        # assigns a unique index to each token.\n",
        "        for index, token in enumerate(self.vocabulary):\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "\n",
        "        # Map the special tokens to their IDs.\n",
        "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
        "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "\n",
        "    def space_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given text on whitespace into tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split on whitespace.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Use re.split such that multiple spaces are treated as a single\n",
        "        # separator.\n",
        "        return re.split(\" +\", text)\n",
        "\n",
        "    def join_text(self, text_list: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string.\n",
        "\n",
        "        The combined tokens, as a single string, are separated by spaces in the\n",
        "        string.\n",
        "\n",
        "        Args:\n",
        "            text_list: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined with a whitespace.\n",
        "\n",
        "        \"\"\"\n",
        "        return \" \".join(text_list)\n",
        "\n",
        "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Create a vocabulary list from the list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of tokens in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of unique tokens (vocabulary) in the dataset.\n",
        "        \"\"\"\n",
        "        return sorted(list(set(tokens)))\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            A list of indices corresponding to the tokens in the input text.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert tokens into indices.\n",
        "        indices = []\n",
        "        unk_index = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "        for token in self.space_tokenize(text):\n",
        "            token_index = self.token_to_index.get(token, unk_index)\n",
        "            indices.append(token_index)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "        Args:\n",
        "            indices: A single index or a list of indices to be\n",
        "                decoded into tokens.\n",
        "\n",
        "        Returns:\n",
        "            A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "\n",
        "        # If a single integer is passed, convert it into a list.\n",
        "        if isinstance(indices, int):\n",
        "            indices = [indices]\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = []\n",
        "        for index in indices:\n",
        "            token = self.index_to_token.get(index, self.unknown_token_id)\n",
        "            tokens.append(token)\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        return self.join_text(tokens)\n",
        "\n",
        "\n",
        "# Initialize the tokenizer. This will build the tokenizer's vocabulary with\n",
        "# all the tokens that appear in the dataset.\n",
        "tokenizer = SimpleWordTokenizer(dataset)\n",
        "\n",
        "# Translate all tokens to their corresponding IDs.\n",
        "encoded_tokens = []\n",
        "for text in dataset:\n",
        "    # Split text into tokens and translate the tokens to token IDs.\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    encoded_tokens.append(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shQvu8OqIuGY"
      },
      "source": [
        "To verify that this process was successful, inspect the first ten token IDs in the first example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWRFQkRFI1df"
      },
      "outputs": [],
      "source": [
        "encoded_tokens[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38uEXAhL6Rmc"
      },
      "source": [
        "## Padding the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2HzW94G7PVG"
      },
      "source": [
        "------\n",
        "> **â„¹ï¸ Info: Padding and truncating**\n",
        ">\n",
        ">The input to transformer models (or deep learning models more generally) is a **matrix** where each row corresponds to the data for one example in the dataset. In the case of the language model you will be training, each paragraph from the Africa Galore dataset constitutes an example. The input should therefore be a matrix that has the IDs of every token in a paragraph. In this matrix, the first entry of a row should be the ID of the first token, the second entry should be the ID of the second token, the third entry should be the ID of the third token, and so on.\n",
        ">\n",
        ">However, the paragraphs in a dataset rarely all have exactly the same length. This causes a problem when you try to combine the data of multiple paragraphs into a matrix, since every row in a matrix must have the same number of entries.\n",
        ">\n",
        ">There are two common solutions to this problem:\n",
        ">1. You can use a special `<PAD>` token to ensure that all sequences have the same length. This way, you can pad shorter paragraphs to match the length of the longest paragraph. This is done by adding `<PAD>` tokens at the beginning or the end of the paragraph. This results in all paragraphs having exactly the same length so that they can be combined in one matrix.\n",
        ">2. Another option is to truncate paragraphs. That is, removing the tokens at the beginning or the end of a paragraph so that they have the length of the shortest paragraph. This, however, may remove a lot of information from the dataset. For example, if the shortest paragraph has only five tokens, then you would shorten every paragraph to five tokens and remove almost all tokens.\n",
        ">\n",
        "> It is also possible to combine both of these methods so that you choose a target length. That way, very long paragraphs that exceed this length are truncated and short paragraphs whose length is below the target length are padded.\n",
        ">\n",
        ">The combination of truncating and padding is what is usually done in practice. You will implement this in the next activity to prepare the data for training the model.\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWA-cd-K_ql3"
      },
      "source": [
        "### Coding Activity 1: Compute length statistics\n",
        "\n",
        "To get a sense of what the dataset looks like and how much padding is needed, compute some statstics of the length of the dataset.\n",
        "\n",
        "First, look at the length of the first paragraph:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e_RgRB6gvL6B",
        "outputId": "27668ea5-6cbb-4d64-9451-2eb2d288e689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of first paragraph: 118\n"
          ]
        }
      ],
      "source": [
        "print(f\"Length of first paragraph: {len(encoded_tokens[0]):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCEiSccJ9Y-3"
      },
      "source": [
        "------\n",
        "> ðŸ’» **Your task:**\n",
        ">\n",
        "> Complete the following cell to compute the length of the shortest paragraph and the length of the longest paragraph.\n",
        ">\n",
        "> There are multiple ways you go about this. For example, you could write a loop that goes through all paragraphs in `encoded_tokens`. You could update variables for the shortest and longest paragraph length whenever you encounter a shorter or longer paragraph than previously seen.\n",
        ">\n",
        "> Alternatively, you can use the `min` and `max` functions in combination with the `len` function in Python. For example, if you have a list of lists `list_of_lists`, then\n",
        ">`min(list_of_lists, key=len)` returns the list in `list_of_lists` with the shortest list (or one of them if there are multiple that have the same length).\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LY2hke_k_LT0",
        "outputId": "4cb23fb5-b84a-4c2a-a63d-32bbebee6fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the shortest paragraph is: 26\n",
            "Length of the longest paragraph is: 318\n"
          ]
        }
      ],
      "source": [
        "# Add your code to compute the length of the shortest paragraph here.\n",
        "shortest_paragraph_length =len(min(encoded_tokens, key=len))\n",
        "\n",
        "# Add your code to compute the length of the longest paragraph here.\n",
        "longest_paragraph_length =len(max(encoded_tokens, key=len))\n",
        "\n",
        "print(f\"Length of the shortest paragraph is:\", shortest_paragraph_length)\n",
        "print(f\"Length of the longest paragraph is:\", longest_paragraph_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your code\n",
        "\n",
        "slm.test_max_min_seqlen(\n",
        "    shortest_paragraph_length, longest_paragraph_length, encoded_tokens\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qVf0wHPuPV-3",
        "outputId": "5b5f7bb3-d3c3-41e8-97ba-2a45497cb65c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Nice! Your answer looks correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5Vsplt_L8b"
      },
      "source": [
        "You can now use this information to set the target length (`max_length`) for padding and truncating the paragraphs in your dataset. The cell below does this behind the scenes using the [`keras.preprocessing.sequence.pad_sequences`](https://github.com/keras-team/keras/blob/v3.10.0/keras/src/utils/sequence_utils.py#L12) function from the Keras package.\n",
        "\n",
        "Change the value below to different values, and observe how the list of token IDs for the first paragraph changes. What happens when you set `max_length` to a very small value? What happens when you set it to the length of the longest paragraph?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "m-AjAUdVvDjm",
        "outputId": "083122a3-87de-40bf-b4aa-ac197c1d9e28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New length of first paragraph: 318 \n",
            "\n",
            "Padding makes the length of all sequences the same as the specified `max_length`.\n",
            "Notice the padded token IDs {tokenizer.pad_token_id} appearing at the end of the sequence.\n",
            "\n",
            "Padded tokens of first paragraph:\n",
            " [ 814  511  985 5092 4802 5183 2800 1363 4792 2134 2856 4792 1584 5092\n",
            " 2088  814 1134 3043 2922  912 2821  170 2623 4792 2023 3807 3576  912\n",
            " 1653 3772 4792 2775 1244  912 4409 3280 1030 4792 1158 3049 1992  912\n",
            " 1868 2486 2437  135 5189 3422  445 3388 2078 4849 4792 3407 2706 1259\n",
            " 4692 2856 4839 5183 4792 4078  814 3406 4259 4849 2389 4831 2707  912\n",
            " 3821 1829 3522 2134 1030 2955  185 1076 2707 3683 5143 1849 4343 1030\n",
            " 1546 1446 4983 2856 4792 2876 4078  814 3406 5092 3366 4788 2968 2151\n",
            " 2938 5092  912 1450 3522 3101  912 1672 4849 4793 4295 2721  912 5036\n",
            " 2224 3522 4792 4437 3522  513    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# @title Set `max_length` for padding and truncating data.\n",
        "\n",
        "max_length = 318  # @param {type: \"number\"}\n",
        "\n",
        "if max_length <= 0:\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<h3>Error:</h3><p>Max length must be greater than 0. Please\"\n",
        "            f\" increase <code>max_length</code>.</p><p></p>\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "elif max_length > longest_paragraph_length:\n",
        "    display(\n",
        "        HTML(\n",
        "            f\"<h3>Error:</h3><p>The padding token <code>\"\n",
        "            f\" {tokenizer.pad_token_id}</code> will be added to all\"\n",
        "            f\" sequences - you probably don't want that. Please reduce\"\n",
        "            f\" <code>max_length</code>.</p><p></p>\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "else:\n",
        "    if max_length < longest_paragraph_length:\n",
        "        display(\n",
        "            HTML(\n",
        "                f\"<p><strong>Note:</strong> The longest paragraph has\"\n",
        "                f\" {longest_paragraph_length} tokens,\"\n",
        "                f\" but <code>max_length</code> is set to {max_length}.\"\n",
        "                f\" Paragraphs longer than <code>max_length</code> will be\"\n",
        "                \" truncated.</p><p></p>\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        encoded_tokens,\n",
        "        maxlen=max_length,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\",\n",
        "        value=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    print(\"New length of first paragraph:\", len(padded_sequences[0]), \"\\n\")\n",
        "\n",
        "    print(\n",
        "        \"Padding makes the length of all sequences the same as the specified\"\n",
        "        \" `max_length`.\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"Notice the padded token IDs {tokenizer.pad_token_id} appearing at the\"\n",
        "        f\" end of the sequence.\\n\"\n",
        "    )\n",
        "    print(\"Padded tokens of first paragraph:\\n\", padded_sequences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81N0QsKuEPhq"
      },
      "source": [
        "## Prepare input and target\n",
        "\n",
        "Recall that the task of a language model is to predict the next token given a context of previous tokens. In the case of n-gram models, you could \"teach\" the model to do this by counting n-grams in the corpus and directly computing the probabilities from the n-gram counts.\n",
        "\n",
        "Transformers have a more involved training procedure. They repeatedly make guesses of what the next token should be. If they get this guess wrong, the training procedure updates the model parameters. That way, the model is more likely to make a correct guess next time.\n",
        "\n",
        "For this training procedure, you have to prepare the data such that you have a separate *input* and *target* dataset:\n",
        "\n",
        "* **Input**: The input is a sequence of tokens that is passed into the transformer model. This may be a part of a paragraph, a full paragraph, or even multiple paragraphs, depending on how the data is structured. The input will contain everything but the last token since there is no next token for the last token.\n",
        "  \n",
        "* **Target**: The target sequence is what you want the model to predict from the input. The target will be the same as the input sequence, but *shifted left* by one token. This means the target will always contain the next token that should follow the input sequence. The target sequence will contain everything but the first word. That is because the transformer always needs at least one token as input, so it will start by predicting the next word.\n",
        "\n",
        "For example, if your dataset consists of the sentence \"Table Mountain is beautiful,\" the corresponding input and target sequences would look as follows:\n",
        "* Input: `[\"Table\", \"Mountain\", \"is\"]` (last token removed).\n",
        "* Target: `[\"Mountain\", \"is\", \"beautiful\"]` (shifted by one token).\n",
        "\n",
        "As mentioned above, the input sequence and the target sequence will actually be the corresponding token IDs instead of the raw tokens. The raw tokens are included here to make the example more readable."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to prepare the input and target sequence for training the transformer model."
      ],
      "metadata": {
        "id": "wfKPIH2uSzix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JLmeuOr2obyr"
      },
      "outputs": [],
      "source": [
        "# Prepare input and target for the transformer model.\n",
        "# For each example, extract all tokens except the last one.\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "# For each example, extract all tokens except the first one.\n",
        "target_sequences = padded_sequences[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc2IrUF-ovZN"
      },
      "source": [
        "To check that the input sequence and the target sequence have been properly prepared, print the first ten tokens in the input and the target sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QCAQlnbrowAz",
        "outputId": "80070127-2a6c-4a3c-cf26-967eac7a2217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 token IDs in first input sequence: [ 814  511  985 5092 4802 5183 2800 1363 4792 2134]\n",
            "First 10 tokens in first input sequence: The Lagos air was thick with humidity, but the energy\n",
            "\n",
            "\n",
            "First 10 token IDs in first target sequence: [ 511  985 5092 4802 5183 2800 1363 4792 2134 2856]\n",
            "First 10 tokens in target sequence: Lagos air was thick with humidity, but the energy in\n"
          ]
        }
      ],
      "source": [
        "print(\"First 10 token IDs in first input sequence:\", input_sequences[0, :10])\n",
        "print(\n",
        "    \"First 10 tokens in first input sequence:\",\n",
        "    tokenizer.decode(input_sequences[0, :10]),\n",
        ")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"First 10 token IDs in first target sequence:\", target_sequences[0, :10])\n",
        "print(\n",
        "    \"First 10 tokens in target sequence:\",\n",
        "    tokenizer.decode(target_sequences[0, :10])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUfcnq9qtErc"
      },
      "source": [
        "You should see in the output above that the target sequence is the input sequence shifted by one token to the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "107RTtKWCpV2"
      },
      "source": [
        "When you set the maximum paragraph length `max_length` previously, you were considering all tokens, including the first and the last token of each paragraph. However, in the `input_sequences`, you removed the first token from each paragraph. In the `target_sequences`, you removed the last token. So, the maximum length of the data in `input_sequences` and `target_sequences` is now one token shorter.\n",
        "\n",
        "Run the following cell to update the `max_length` variable. This variable will be used as a parameter of the transformer model and needs to accurately reflect what the length of each (padded) paragraph in your input data is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NWSao54yCk9M"
      },
      "outputs": [],
      "source": [
        "max_length = input_sequences.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUty-eHlCTXy"
      },
      "source": [
        "## Shuffle the dataset and specify the batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NutUoSXCSMQI"
      },
      "source": [
        "\n",
        "-------\n",
        "> **â„¹ï¸ Info: The purpose of shuffling and batches**\n",
        ">\n",
        ">The final step before you can train your small language model is to split the data into groups of a handful of paragraphs, called **batches**. Furthermore, there is often some order in your data. For example, in the Africa Galore dataset, all examples concerning food appear one after each other. When training a model, however, it is generally best to include a very diverse set of paragraphs in one batch. This can be achieved by **shuffling** the data in the dataset so that all paragraphs appear in random order before splitting them up into batches. Note that the order of tokens within a paragraph must remain intact since you would end up with word puzzles otherwise.\n",
        ">\n",
        ">For splitting the dataset into batches, you need to define the **batch size**, that is, the number of paragraphs that should be included in one batch. Increasing the batch size usually speeds up training of the model and can also lead to better models. At the same time, however, larger batch sizes require more memory. If you set the batch size too large, you may get \"out of memory\" errors that indicate that you do not have enough memory available to train the model. You will learn more about dealing with methods for reducing memory in later courses.\n",
        ">\n",
        ">The figure below shows a dataset with seven paragraphs. Each paragraph is padded to `max_length`. In this case, it is set to the length of the longest paragraph. The dataset is then shuffled and split into batches of size 3. Note that the final batch only contains one paragraph, since the total number of paragraphs is 7 and not divisible by 3.\n",
        "> <img src='https://storage.googleapis.com/dm-educational/assets/ai_foundations/evolve_graphic.png' width='1000'>\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yazOaI7XVlx"
      },
      "source": [
        "The cell below implements the shuffling of the dataset and splitting it into batches. The result of this is a list of matrices referred to as **tensors**. Each matrix corresponds to a batch and contains all the token IDs for all paragraphs in that batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WQg55VpOw-2j",
        "outputId": "6d3c1a6d-8189-4a6c-97c3-ccf7c50dc83f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 317), dtype=int32, numpy=\n",
            "array([[ 719, 5092, 4815, ...,    0,    0,    0],\n",
            "       [ 797,  597,  912, ...,    0,    0,    0],\n",
            "       [ 470, 4084, 2932, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 814, 4079, 1171, ...,    0,    0,    0],\n",
            "       [ 814, 3085, 2932, ...,    0,    0,    0],\n",
            "       [ 358, 1605, 2935, ...,    0,    0,    0]], dtype=int32)>, <tf.Tensor: shape=(32, 317), dtype=int32, numpy=\n",
            "array([[5092, 4815, 4403, ...,    0,    0,    0],\n",
            "       [ 597,  912, 2364, ...,    0,    0,    0],\n",
            "       [4084, 2932,  912, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [4079, 1171, 3522, ...,    0,    0,    0],\n",
            "       [3085, 2932, 4792, ...,    0,    0,    0],\n",
            "       [1605, 2935, 2968, ...,    0,    0,    0]], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "# Create TensorFlow dataset to prepare sequences.\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "\n",
        "# Randomly shuffle the dataset.\n",
        "# The buffer_size determines how many examples from the dataset\n",
        "# are held in memory before shuffling.\n",
        "# If you are working with a very large dataset,\n",
        "# reduce the buffer_size as needed.\n",
        "tf_dataset = tf_dataset.shuffle(buffer_size=len(input_sequences))\n",
        "\n",
        "# Specify batch size.\n",
        "batch_size = 32  # @param {type: \"number\"}\n",
        "\n",
        "# Create batches.\n",
        "batches = tf_dataset.batch(batch_size)\n",
        "\n",
        "for batch in batches.take(1):\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVELGmoNmR3g"
      },
      "source": [
        "Run the following cell to count the total number of batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0sc2lVvlktGH",
        "outputId": "dd57fea3-388d-4cce-dc5e-1c12a964bee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of batches is: 8\n"
          ]
        }
      ],
      "source": [
        "total_batches = 0\n",
        "for batch in batches:\n",
        "    total_batches += 1\n",
        "print(\"Total number of batches is:\", total_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nwdEqyCfA_t"
      },
      "source": [
        "## Train a small language model (SLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4dsdg-DtW64"
      },
      "source": [
        "You have now done all the preparatory work and are ready to train your small language model. As mentioned above, this model has around 3.5 million parameters. It is therefore  a lot smaller than so-called large language models that are used in production. For example, the Google Gemini model has billions of parameters and was trained on a much bigger dataset than the Africa Galore dataset.\n",
        "\n",
        "The size of the transformer model and the amount of training data has a strong impact on its performance. Larger models with more parameters have the capacity to learn more complex patterns and deliver better accuracy. However, they also require more computational resources, memory, and processing power. This can lead to longer training times i.e., how long the model needs to update to reach optimal performance, and higher costs. You would not be able to train a very large model in a Colab notebook. Therefore, you will be training a much smaller model here. Despite this, the overall process for training a large language model is the same as the process for training a small language model.\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Parameters of a transformer model**\n",
        ">\n",
        "> **Parameters** are a set of numbers that guide the model to perform whatever task it was trained to do. In the case of transformer models, the parameters are less interpretable. They are often a very large collection of numbers that determine the model behavior.\n",
        ">\n",
        "> The parameters of a transformer model are updated after processing each batch of paragraphs. At the start of the training, the parameters are intialized with random numbers.\n",
        ">Models are then usually trained by processing the data multiple times. Going through the data once is known as an **iteration** or **epoch**. During each training iteration, the parameters are updated so that they lead to better predictions of the next token.\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZAHvAaAAvKI"
      },
      "source": [
        "### Initialize the model\n",
        "\n",
        "The `create_model` function used below builds a transformer model. It takes two parameters:\n",
        "\n",
        "* `max_length`: The maximum length of a paragraph in the dataset (which you set above). The model will only be able to process sequences up to this length.\n",
        "* `vocabulary_size`: The size of the vocabulary. That is the number of unique tokens in the dataset. This is used in two ways. Firstly, it is used to determine the number of unique inputs the model should expect. Secondly, it determines how many different tokens the model can predict. You can get this information from the tokenizer that you defined above by using its `vocabulary_size` property.\n",
        "* `learning_rate`: How quickly the parameters should be updated. Setting this to a higher value can speed up training but may result in a worse model. Setting this to a lower value likely improves how the model learns but may slow down training. For now, you do not have to change this value and you will learn more about this setting in later courses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNGqssFqlzqE"
      },
      "outputs": [],
      "source": [
        "model = training.create_model(\n",
        "    max_length=max_length,\n",
        "    vocabulary_size=tokenizer.vocabulary_size,\n",
        "    learning_rate=1e-4\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVA4N5N9nAwB"
      },
      "source": [
        "### Initialize a callback function\n",
        "\n",
        "Training can take a while. You want to make sure that the model predictions actually get better over time. One way to do this is to define a **callback function** that is used to regularly print what the model would generate for one prompt.\n",
        "\n",
        "For example, the callback function defined in the following cell will print ten tokens for the prompt \"Abeni,\" after every 10 training iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1GhWqSRVm_ql"
      },
      "outputs": [],
      "source": [
        "prompt = \"Abeni,\"\n",
        "prompt_ids = tokenizer.encode(prompt)\n",
        "text_gen_callback = training.TextGenerator(\n",
        "    max_tokens=10, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P_McuozGX6R"
      },
      "source": [
        "### Run the training\n",
        "\n",
        "Run the following cell to train the model. As mentioned above, the training process updates the model parameter after processing each batch. This is known as a step in the training process.\n",
        "\n",
        "An epoch involves processing all batches in the dataset. Before training the model, you have to set the number of times the training process should process the datset. This is done by setting the number of epochs (`num_epochs`).\n",
        "\n",
        "You will likely get the best results if you train the model for at least 200 epochs. But if training is taking a long time, you can reduce the number of epochs. If the model does not perform well after 200 epochs, you can train it for additional epochs by adjusting the number below and re-running the cell. This will continue training your model.\n",
        "\n",
        "If you want to reset the training, re-run the previous cells before running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiJciyOo5gYU"
      },
      "outputs": [],
      "source": [
        "num_epochs = 200  # @param {type: \"number\"}\n",
        "# verbose=2: Instructs the model.fit method to print one line per\n",
        "# epoch so you see how the loss is decreasing and generated texts improving.\n",
        "history = model.fit(\n",
        "    x=batches, verbose=2, epochs=num_epochs, callbacks=[text_gen_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf1BR0cGoy0V"
      },
      "source": [
        "While the model is training, you can observe how the generated text changes. At the beginning of the training, the generation will likely be a random collection of words. By the end of the training, however, the generation should start to become more coherent.\n",
        "\n",
        "Apart from observing how the generated text changes, you can also check how the **loss** changes as training progresses. If the model is training properly, the loss should go down as training continues. You may find that the loss temporarily goes up again from one epoch to another. This is nothing to worry about, but the general trend should be that the loss descreases.\n",
        "\n",
        "Once the training process has finished (this can take some time), you can prompt the model as you did in earlier labs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTbezoZr5VKc"
      },
      "source": [
        "#### Evaluate your small language model\n",
        "\n",
        "After training a model, researchers have to perform many evaluations to determine whether it is performing well in many scenarios.\n",
        "\n",
        "As a final activity, you will also evaluate your model. The remainder of this lab guides you through this evaluation process. You will ask the following key questions to evaluate your model's quality:\n",
        "\n",
        "*   A. How good is your model at predicting the next token for a given prompt based on patterns identified in the training dataset?\n",
        "*   B. Is the generated text coherent, and does it make sense given the context?\n",
        "*   C. Is the likely next token what you expect to see when the context is changed slightly?\n",
        "\n",
        "When evaluating your model, you may find it useful to take some notes. To do this, you can either add cells to this Colab notebook or take notes on [Google Docs](https://docs.google.com/), [Notebook LM](https://notebooklm.google/), a piece of paper, or any other note-taking tool of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukARAmZpEUOJ"
      },
      "source": [
        "### How good is your model at predicting the next token for a given prompt based on patterns identified in the training dataset?\n",
        "\n",
        "The following steps provide you with some guidance on how to answer this question.\n",
        "\n",
        "* Prompt the model using a token or sequence of tokens from the training dataset. For example, you can start with `\"Abeni, a bright-eyed\"`.\n",
        "* Visualize the probability distribution of the next token for a given prompt.\n",
        "* Increase `num_tokens_to_generate` to generate longer texts.\n",
        "* Inspect the generated text. See how well the model has learned to generate text that reflects the patterns learned during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E06Mp-wFC8q"
      },
      "outputs": [],
      "source": [
        "prompt = \"Abeni, a bright-eyed\" #@param {type: \"string\"}\n",
        "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
        "generated_text, probs = generation.generate_text(\n",
        "    prompt,\n",
        "    num_tokens_to_generate,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sampling_mode=\"greedy\" # To generate the highest probability generation.\n",
        ")\n",
        "\n",
        "print(\"Generated text:\", generated_text)\n",
        "print(\"\\n\")\n",
        "\n",
        "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXz3Wu-10jlX"
      },
      "source": [
        "### Is the generated text coherent, and does it make sense given the context?\n",
        "* Prompt the model with a token  or a phrase of your choosing.\n",
        "* Increase `num_tokens_to_generate` to generate longer texts.\n",
        "* Visualize the probability distribution of the next token for a given prompt.\n",
        "* Inspect the quality of generated texts.\n",
        "\n",
        "Note that, above, the generation process always chooses the most probable next token from the set of candidate tokens. In the next cell, the generation process samples a next token according to the probability distribution predicted by the model. This is done by setting the `sampling_mode` parameter to `random`.\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Unseen tokens**\n",
        ">\n",
        ">When you are trying different prompts, you may also notice that sometimes tokens get replaced by the special string `<UNK>`. This happens when you prompt the model with tokens that did not appear in the training dataset, so called **unseen tokens**. For these tokens, the `token_to_index` dictionary of the tokenizer does not have an entry and therefore they cannot be mapped to a token index.\n",
        ">\n",
        "> One method of dealing with such tokens is to add a special `<UNK>` token along with its index to the vocabulary of the tokenizer. Then, during **inference**, whenever there is an unseen token, it maps the token to the index of this special `<UNK>` token.\n",
        ">\n",
        "> This method is not ideal because all information in the token is lost. In later courses, you will learn more sophisticated methods of dealing with unseen tokens that do not rely on such a catch-all token. For now, you will likely observe that the model is not very good at predicting the next word if there are several unseen words in the prompt.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1riRfT3lbA8"
      },
      "outputs": [],
      "source": [
        "prompt = \"Jide was hungry so she went looking for\" #@param {type: \"string\"}\n",
        "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
        "generated_text, probs = generation.generate_text(\n",
        "    prompt,\n",
        "    num_tokens_to_generate,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sampling_mode=\"random\",\n",
        ")\n",
        "print(\"Generated text:\", generated_text)\n",
        "print(\"\\n\")\n",
        "\n",
        "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEoUkDVFd_0M"
      },
      "source": [
        "### Is the likely next token what you expect to see when the context is changed slightly?\n",
        "* Change the context of the prompt slightly.\n",
        "* Visualize the probability distribution of the next token for a given prompt.\n",
        "* Increase `num_tokens_to_generate` to generate longer texts.\n",
        "* Inspect the quality of generated texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHFdQf4Fdn9x"
      },
      "outputs": [],
      "source": [
        "prompt = \"Jide was thirsty so she went looking for\" #@param {type: \"string\"}\n",
        "num_tokens_to_generate = 10 #@param {type: \"number\"}\n",
        "generated_text, probs = generation.generate_text(\n",
        "    prompt,\n",
        "    num_tokens_to_generate,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    sampling_mode=\"random\",\n",
        ")\n",
        "\n",
        "print(\"Generated text:\", generated_text)\n",
        "print(\"\\n\")\n",
        "\n",
        "visualizations.plot_next_token(probs[0], prompt=prompt, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHrgLwtmoY2w"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the **Train your own small language model** lab.\n",
        "\n",
        "In this lab, you trained your first SLM and engaged in the following steps.\n",
        "\n",
        "- **Tokenized the dataset:** You used the `SimpleWordTokenizer` from the previous lab to tokenize and convert the paragraphs in the dataset to token IDs.\n",
        "\n",
        "- **Padded the paragraphs:** You ensured all paragraphs had the same length by truncating some of them and padding others with a special `\"<PAD>\"` token. This is crucial for processing data in neural networks, such as transformer language models.\n",
        "\n",
        "- **Prepared the input and target data:** You created input-target pairs, where the target is the input sequence shifted by one token. This teaches the model to predict the next token based on the context of previous tokens.\n",
        "\n",
        "- **Shuffled and batched the data:** You shuffled the dataset to increase the diversity of the data within each batch and grouped the paragraphs into batches for training.\n",
        "\n",
        "- **Trained the SLM:** You defined and trained a small transformer model, observing how the training loss decreased during training.\n",
        "\n",
        "- **Prompted the trained model:** You experimented with prompting the model, observing its ability to predict the likely next word, generate coherent text, and adapt to changes in context.\n",
        "\n",
        "As you performed your evaluations, you may have noticed that some of the model predictions are not as good as the ones that you have seen with the Gemma model. This is expected since your model is a lot smaller than the Gemma model. It has been trained on *a lot* less text data. Nevertheless, your model should be able to produce grammatical sentences, even if they do not always make a lot of sense.\n",
        "\n",
        "In the next section of the course, you will explore model evaluation in a little more depth. You will then move on to think about the kinds of problems you are interested in using language models to address."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities above. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "However, we recommend that you *only* look at the solutions after you have tried to solve the activities above *multiple times*. The best way to learn challenging concepts in computer science and artifical intelligence is to debug your code piece by piece until it works rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code, for example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also make you practice how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them and then type them manually into the cell. This will help you understand where you went wrong."
      ],
      "metadata": {
        "id": "TbCry1ll6L9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "4NJgcGxO6UDE"
      }
    },
    {
      "metadata": {
        "id": "UD38Eu8K7sXg"
      },
      "cell_type": "code",
      "source": [
        "# Add this code in the cell for Activity 1 above.\n",
        "longest_paragraph_length = len(max(encoded_tokens, key=len))\n",
        "shortest_paragraph_length = len(min(encoded_tokens, key=len))\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "TbCry1ll6L9g",
        "4NJgcGxO6UDE"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}